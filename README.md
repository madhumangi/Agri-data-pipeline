# ğŸŒ¾ Agri Data Pipeline

A production-grade data pipeline to process, transform, validate, and store agricultural sensor data using Python, Pandas, and DuckDB.

---

## ğŸ“ Folder Structure

```bash
agri-data-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Input Parquet files
â”‚   â”œâ”€â”€ processed/          # Output partitioned Parquet files
â”‚   â””â”€â”€ data_quality_report.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion.py        # Read & validate raw data
â”‚   â”œâ”€â”€ transformation.py   # Clean, enrich, normalize
â”‚   â”œâ”€â”€ validation.py       # DuckDB-based validation
â”‚   â””â”€â”€ loading.py          # Save output Parquet files
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_transformation.py
â”‚   â”œâ”€â”€ test_validation.py
â”‚   â””â”€â”€ test_loading.py
â”œâ”€â”€ generate_sample_data.py # Script to create test Parquet files
â”œâ”€â”€ pipeline.py             # Main pipeline script
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ Dockerfile              # For containerized execution
â””â”€â”€ README.md
```

---

## ğŸ§ª Sample Schema

Each `.parquet` file in `data/raw/` contains:

| Column         | Type     | Description              |
| -------------- | -------- | ------------------------ |
| sensor\_id     | string   | Unique ID for the sensor |
| timestamp      | datetime | ISO timestamp (UTC)      |
| reading\_type  | string   | temperature / humidity   |
| value          | float    | Sensor value             |
| battery\_level | float    | Battery %                |

---

## âš™ï¸ Pipeline Stages

### 1. **Ingestion**

* Reads `.parquet` files from `data/raw/`
* Validates schema, handles corrupt files

### 2. **Transformation**

* Drops duplicates, missing values, outliers
* Flags anomalies based on expected ranges:

  * `temperature`: 0â€“60Â°C
  * `humidity`: 10â€“90%
* Applies calibration logic:

  ```python
  value = value * multiplier + offset
  ```
* Computes:

  * Daily average
  * 7-day rolling average

### 3. **Validation**

* Uses DuckDB SQL to:

  * Profile anomalies and missing data
  * Detect hourly gaps per sensor
* Saves result to `data/data_quality_report.csv`

### 4. **Loading**

* Writes cleaned data to `data/processed/`
* Partitioned by `date`, compressed using `snappy`

---

## ğŸš€ How to Run

### 1. Install requirements

```bash
pip install -r requirements.txt
```

### 2. Generate sample input files

```bash
python generate_sample_data.py
```

### 3. Run the pipeline

```bash
python pipeline.py
```

---

## ğŸ³ Docker Setup

### Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "pipeline.py"]
```

### Build & Run

```bash
docker build -t agri-pipeline .
docker run -v $(pwd)/data:/app/data agri-pipeline
```

On Windows (PowerShell):

```bash
docker run -v ${PWD}/data:/app/data agri-pipeline
```

---

## ğŸ“Š Output Files

* Cleaned `.parquet` files in: `data/processed/` (partitioned by `date`)
* Data quality CSV: `data/data_quality_report.csv`

---

## ğŸ“Š Example: Data Quality Report Output

The pipeline generates a data quality report using DuckDB and saves it to:

```
data/data_quality_report.csv
```

### Sample Content:

| reading\_type | total\_records | anomalous\_count | pct\_anomalous | missing\_values | sensor\_id | actual\_hours | missing\_hours |
| ------------- | -------------- | ---------------- | -------------- | --------------- | ---------- | ------------- | -------------- |
| temperature   | 20             | 2                | 10.0%          | 0               | sensor\_1  | 18            | 6              |
| humidity      | 20             | 1                | 5.0%           | 0               | sensor\_3  | 19            | 5              |

This report provides visibility into missing values, anomaly rates, and coverage gaps in hourly readings.

---


